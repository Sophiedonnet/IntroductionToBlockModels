mat
as.array(mat)
a[1]
a <- as.array(mat)
a[1]
x <- as.array(matrix(1:4, 2, 2))
mx.ctx.default(mx.cpu(1))
print(mx.ctx.default())
print(is.mx.context(mx.cpu()))
mat <- mx.nd.array(x)
mat <- (mat * 3 + 5) / 10
as.array(mat)
### LOAD TRAIN
n = 50000
labels = rep(NA,n)
labels = rep(NA,n)
gc(reset=T)
to.read = file(paste(CifarDir,"/cifar-100-binary/train.bin",sep=""), "rb")
v = readBin(to.read, "integer",n=n*3074, size=1 , signed=F)
rm(to.read)
# Extract images in an array
train.array = array(0,dim = c(32,32,3,n))
# Extract images in an array
train.array = array(0,dim = c(32,32,3,n))
for(i in 1:n){
TMP = NULL
#fine[i] = v[(i-1)*3074 +2 ]
labels[i] = v[(i-1)*3074 + 1]
for(ch in 1:3){
tmp = v[(i-1)*3074 + 2 + (1:1024) + (ch - 1 )*1024]
Mtmp= as.vector(matrix(tmp,32,32))
TMP=c(TMP,Mtmp)
train.array[,,,i] = array(TMP , dim=c(32,32,3))
}
}
# Plot a training image
par(mfrow = c(4,5))
examples = NULL
for(i in 1:20){
examples[i]= which(labels==(i-1))[1]
plot.RGBarray(train.array[,,,examples[i]])
}
# Plot a training image
par(mfrow = c(2,2))
examples = NULL
for(i in 1:4){
examples[i]= which(labels==(i-1))[1]
plot.RGBarray(train.array[,,,examples[i]])
}
# Make TrAIN label array
Y.train = array(0,dim=c(length(unique(labels)),length(labels)))
for(i in 1:length(labels)){Y.train[labels[i]+1,i] = 1}
Y.train
# Subsample data and make VALIDATION SET
# Validation set = 2% of whole train
nTrain = 49000
bag = 1:length(labels)
sampled = sample(bag,nTrain)
Xtrain =train.array[,,,sampled,drop=F]
labtrain = Y.train[,sampled,drop=F]
validSelec = setdiff(1:length(labels),sampled)
Xvalid =train.array[,,,validSelec,drop=F]
Yvalid = Y.train[,validSelec,drop=F]
### LOAD TEST
n = 10000
test.labels.vect = rep(NA,n)
gc(reset=T)
to.read = file(paste(CifarDir,"/cifar-100-binary/test.bin",sep=""), "rb")
v = readBin(to.read, "integer",n=n*3074, size=1 , signed=F)
# Extract images in an array
test.array = array(0,dim = c(32,32,3,n))
for(i in 1:n){
TMP = NULL
test.labels.vect[i] = v[(i-1)*3074 + 1]
for(ch in 1:3){
tmp = v[(i-1)*3074 + 2 + (1:1024) + (ch - 1 )*1024]
Mtmp= as.vector(matrix(tmp,32,32))
TMP=c(TMP,Mtmp)
test.array[,,,i] = array(TMP , dim=c(32,32,3))
}
}
# Make TrAIN label array
Y.train = array(0,dim=c(length(unique(labels)),length(labels)))
for(i in 1:length(labels)){Y.train[labels[i]+1,i] = 1}
labels
nTrain = 49000
bag = 1:length(labels)
sampled = sample(bag,nTrain)
Xtrain =train.array[,,,sampled,drop=F]
labtrain = Y.train[,sampled,drop=F]
validSelec = setdiff(1:length(labels),sampled)
Xvalid =train.array[,,,validSelec,drop=F]
Yvalid = Y.train[,validSelec,drop=F]
### LOAD TEST
n = 10000
test.labels.vect = rep(NA,n)
gc(reset=T)
# 1st convolutional layer
conv1 = mx.symbol.Convolution(data = data, kernel = c(5, 5),
pad = c(2, 2),
num_filter = 64,
name = "conv1")
n_labels = dim(Y.train)[1]
data = mx.symbol.Variable(name = "data")
label = mx.symbol.Variable(name="label")
# 1st convolutional layer
conv1 = mx.symbol.Convolution(data = data, kernel = c(5, 5),
pad = c(2, 2),
num_filter = 64,
name = "conv1")
relu1 = mx.symbol.Activation(data = conv1,
act_type = "relu",
name = "relu1")
pool1 = mx.symbol.Pooling(data = relu1,
pool_type = "max",
kernel = c(4,4),
stride=c(4,4),
name = "pool1")
bn1   = mx.symbol.BatchNorm(name="bn1",
data=pool1,
fix_gamma=F,
momentum = 0.9,
eps = 2e-5)
# 2nd convolutional layer
conv2 = mx.symbol.Convolution(data = bn1,
kernel = c(5, 5),
pad = c(2, 2),
num_filter = 100,
name = "conv2")
relu2 = mx.symbol.Activation(data = conv2,
act_type = "relu",
name = "relu2")
pool2 = mx.symbol.Pooling(data = relu2,
pool_type = "max",
kernel = c(8, 8),
stride =c(8, 8),
name = "pool2")
# Flattening
flatten = mx.symbol.Flatten(data = pool2,
name = "flatten")
bn2   = mx.symbol.BatchNorm( name="bn2",
data=flatten,
fix_gamma=F,
momentum=0.9 ,
eps= 2e-5)
fc1 = mx.symbol.FullyConnected(data = bn2,
num_hidden = 300,
name = "fc1")
rel3 = mx.symbol.Activation(data = fc1,
act_type = "relu",
name = "relu3")
bn4   = mx.symbol.BatchNorm( name="bn4",
data=rel3,
fix_gamma=F,
momentum=0.9 ,
eps= 2e-5)
data = mx.symbol.Variable(name = "data")
label = mx.symbol.Variable(name="label")
# 1st convolutional layer
conv1 = mx.symbol.Convolution(data = data, kernel = c(5, 5),
pad = c(2, 2),
num_filter = 64,
name = "conv1")
relu1 = mx.symbol.Activation(data = conv1,
act_type = "relu",
name = "relu1")
pool1 = mx.symbol.Pooling(data = relu1,
pool_type = "max",
kernel = c(4,4),
stride=c(4,4),
name = "pool1")
bn1   = mx.symbol.BatchNorm(name="bn1",
data=pool1,
fix_gamma=F,
momentum = 0.9,
eps = 2e-5)
# 2nd convolutional layer
conv2 = mx.symbol.Convolution(data = bn1,
kernel = c(5, 5),
pad = c(2, 2),
num_filter = 100,
name = "conv2")
relu2 = mx.symbol.Activation(data = conv2,
act_type = "relu",
name = "relu2")
pool2 = mx.symbol.Pooling(data = relu2,
pool_type = "max",
kernel = c(8, 8),
stride =c(8, 8),
name = "pool2")
# Flattening
flatten = mx.symbol.Flatten(data = pool2,
name = "flatten")
bn2   = mx.symbol.BatchNorm( name="bn2",
data=flatten,
fix_gamma=F,
momentum=0.9 ,
eps= 2e-5)
# Linear Predictor
out = mx.symbol.FullyConnected(data = bn4, num_hidden = n_labels, name = "out")
# log-Loss
QuickLoss = mx.symbol.SoftmaxOutput(data = out , label=label , multi.output = T, name="cross_entropy")
modelSymbols = list(loss=QuickLoss)
# Visualize model
graph = graph.viz(out,type="graph",direction="LR")
print(graph)
softmax = mx.symbol.softmax(data= out , axis=1,  name="softmax")
loss= mx.symbol.MakeLoss(data= 0  - label * mx.symbol.log( softmax ) , name="cross_entropy_custom")
modelSymbols = list(loss=loss,pred=softmax)
AutoSymb= mx.symbol.SoftmaxOutput(data=out)
# Network Weights initialization
mx.set.seed(2019)
model <- mx.model.FeedForward.create(symbol=AutoSymb,
X=Xtrain,
y=labtrain,
ctx=MainDevice,
begin.round=1,
num.round=1,
array.batch.size=50,
learning.rate=0,
initializer = mx.init.uniform(0.03))
gc(reset=T)
n_it = 10
metrics = data.frame(it=0:n_it,train.accuracy=NA,valid.accuracy=NA,train.loss=NA,valid.loss=NA)
pred= predict( model , Xvalid )
samp <- sample(1:dim(labtrain)[2],1000)
samp <- sample(1:dim(labtrain)[2],1000)
metrics$train.accuracy[1] = mean(colSums(labtrain[,samp] * pred))
pred= predict( model , Xtrain[,,,samp] )
metrics$train.loss[1] = mean( - labtrain[,samp] * log(pred) )
# Continue training
for(i in 1:n_it){
model <- mx.model.FeedForward.create(symbol=model$symbol,
arg.params = model$arg.params,
X=Xtrain,
y=labtrain,
ctx=MainDevice,
begin.round=1,
num.round=1,
optimizer ="sgd",
array.batch.size=32,
learning.rate=5e-9,
eval.data = list(data=Xvalid,label=Yvalid),
momentum=0.9
#                                     eval.metric= mx.metric.custom(name = "logLoss",log.loss.metric)
#                                     batch.end.callback=mx.callback.save.checkpoint("wrapper")
)
gc(reset=T)
# Store metrics
cd = metrics$it==i
pred= predict( model , Xtrain[,,,sample(1:dim(labtrain)[2],1000)] )
metrics$train.accuracy[cd] = mean(colSums(Yvalid * pred))
metrics$train.loss[cd] = mean( - Yvalid * log(pred) )
pred= predict( model , Xvalid )
metrics$valid.accuracy[cd] = mean(colSums(Yvalid * pred))
metrics$valid.loss[cd] = mean( - Yvalid * log(pred) )
plot.metrics(metrics,i)
}
warnings()
#####
# 8) Lean CNN with low levels functions
#####
# Training parameters
n_it= 500
batch.size = 32
saveDir = CifarDir
lr = rep(5e-6,n_it+1)
modelName = "finish2"
loadModel = F
if(loadModel ==F){
# Randomly Initialize the model weights
mx.set.seed(2019)
model= mx.model.FeedForward.create(symbol=modelSymbols$loss[[1]],
X=Xtrain,y=labtrain,
ctx=mx.cpu(),begin.round=1,num.round = 1,
learning.rate=0.,
initializer = mx.init.uniform(0.03),
array.layout = "colmajor")
}else{
# OR load pre-trained model
setwd(saveDir)
model = mx.model.load("finish2",iteration = 6)
}
# Initialize the moving gradient average (For SGD/momentum algorithm) to 0
Names = names(model$arg.params)
E_G = lapply(Names,function(arg) 0.*as.array(model$arg.params[arg][[1]]) )
names(E_G)=Names
# Table of loss results per epoch for plot
tab = data.frame( it = 0:n_it,tr.loss = NA,valid.loss=NA)
# prediction sample
N = dim(labtrain)[2]
samplo = sample(1:N,500)
priorLoss = - log(1/ dim(labtrain)[1]) /dim(labtrain)[1] # prior Loss for plot
n.batch = -( -N %/% batch.size)
it=0
while (it<=n_it){
print(paste('it :',it))
# PREDICT on train
p = predict.executor(modelSymbols$pred[[1]],
model$arg.params,
Xtrain[,,,samplo,drop=F],
devices = MainDevice,
aux.params = model$aux.params,
max.compute.size = 200)
# COMPUTE MEAN TRAIN LOSS
loss = loss.executor(predictor.symbol = modelSymbols$loss[[1]],
params =model$arg.params,
Array = Xtrain[,,,samplo,drop=F],
y = labtrain[,samplo,drop=F],
devices = MainDevice,
aux.params = model$aux.params,
max.compute.size = 200)
if(sum(is.na(loss))>0){loss[is.na(loss)] = 0}
if(sum(is.infinite(loss))>0){
print('Infinite terms in the train loss')
tab$tr.loss[tab$it==it]= NA
}else{
meanLoss = mean(as.vector(loss))
print(paste('Mean train Loss :',meanLoss))
tab$tr.loss[tab$it==it]= meanLoss
}
# PREDICT on validation
p = predict.executor(modelSymbols$pred[[1]],
model$arg.params,
Xvalid,
devices = mx.cpu(),
aux.params = model$aux.params,
max.compute.size = 200)
# COMPUTE MEAN VALIDATION LOSS
loss = loss.executor(predictor.symbol = modelSymbols$loss[[1]],
params =model$arg.params,
Array = Xvalid,
y = Yvalid,
devices = mx.cpu(),
aux.params = model$aux.params,
max.compute.size = 200)
if(sum(is.na(loss))>0){loss[is.na(loss)] = 0}
if(sum(is.infinite(loss))>0){
print('Infinite terms in the validation loss')
tab$valid.loss[tab$it==it]= NA
}else{
meanLoss = mean(as.vector(loss))
print(paste('Mean validation Loss :',meanLoss))
tab$valid.loss[tab$it==it]= meanLoss
}
# PLOT LOSS
trainLoss = tab$tr.loss[tab$it<=it]
validLoss = tab$valid.loss[tab$it<=it]
ylim_r=range(c(trainLoss,0,priorLoss,validLoss),na.rm = T)
d=data.frame(its =c(0:it,0:it,0,it,0,it),
val=c(trainLoss,validLoss,0,0,priorLoss,priorLoss),
curve= c(rep("train loss",length(trainLoss)),rep("validation loss",length(validLoss)),"saturated loss","saturated loss","prior loss","prior loss"))
pl=ggplot(d,aes(x=its,y=val,group=curve,colour=curve))+geom_point()+geom_line()+coord_cartesian(ylim = ylim_r) +ylab('Mean train loss')+xlab('Number of epochs')+theme_bw()
print(pl)
# Tirage des mini-batchs
sampli = 1:N
batchs.list = list()
for(k in 1:n.batch){
reduce = (length(sampli)<batch.size) * (batch.size-length(sampli))
batchs.list[[k]] = sample(sampli , batch.size - reduce , replace=F)
sampli = sampli[!(sampli%in% batchs.list[[k]]) ]
}
executor = mx.simple.bind(model$symbol,
data=dim(Xtrain[,,,batchs.list[[1]],drop=F]),
grad.req ="write",
ctx=MainDevice)
# pass over each mini-batch
for(k in 1:n.batch){
elems = batchs.list[[k]]
if(length(elems)<batch.size){
executor = mx.simple.bind(model$symbol,data=dim(Xtrain[,,,elems,drop=F]),grad.req ="write",ctx=MainDevice)
}
cat('\r  Process ...',100*k/n.batch,' %                               \r')
flush.console()
GradAndAux = get.all.gradients(model,Xtrain[,,,elems,drop=F],labtrain[,elems,drop=F],executor)
for(arg in names(GradAndAux[[2]])){
# Update each auxiliary parameter set by extracting them from GradAndAux
# We copy those components to the CPU
# because that is where our model components are located
model$aux.params[arg][[1]] = mx.nd.copyto(GradAndAux[[2]][arg][[1]],mx.cpu())
}
G= GradAndAux[[1]]
for(arg in Names){
# We compute parameters delta through R
# because I had a crash with Mxnet from an unknown reason ...
E_G[arg][[1]] = 0.9 * E_G[arg][[1]] + lr[it+1]* as.array(G[arg][[1]])
# We compute parameters update through Mxnet
model$arg.params[arg][[1]] = model$arg.params[arg][[1]] -  mx.nd.array( E_G[arg][[1]],ctx=mx.cpu())
}
gc(reset=T)
}
it=it+1
setwd(saveDir)
mx.model.save(model,modelName,iteration=it)
write.table(tab,paste(modelName,"_losses.csv",sep=""),sep=";",row.names=F,col.names=T)
}
packages <- c("blogdown", "xaringan", "pagedown", "flextable", "ggiraph", "shiny")
install.packages(packages, repos = "https://cran.rstudio.com")
euca<-read.table("eucalyptus.txt",header=T,sep=" ")
str(euca)
names(euca)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
mod1 = lm(ht~circ:bloc,data=euca) #le  modÃ¨le complet
anova(mod1,mod2)
setwd("D:/WORK_ALL/RECHERCHE/FORMATION_RECHERCHE/FORMATION_RESEAUX/2020_03_ECOSTAT/IntroductionToBlockModels/TP_SBM/pgm")
## ----import dataset,  echo = TRUE--------------------------------------------------------------------------------------------------------------
rm(list=ls())
load('fungi_tree_data.Rdata')
ls()
fungi_tree
dim(fungi_tree)
rm(list=ls())
load('fungi_tree_data.Rdata')
ls()
## ----load pack,  echo = TRUE,message=FALSE-----------------------------------------------------------------------------------------------------
library(ggplot2)
library(igraph)
library(blockmodels)
library(alluvial)
## ----source function,  echo = TRUE, message=FALSE----------------------------------------------------------------------------------------------
source('function_for_blockmodels.R')
## ----tree tree bin plot, echo=TRUE-------------------------------------------------------------------------------------------------------------
plotMatrix(Mat = tree_bin,rowFG = 'tree', colFG  = 'tree')
## ----SBM, echo=TRUE, eval = TRUE---------------------------------------------------------------------------------------------------------------
sbm.tree_bin <- BM_bernoulli("SBM_sym",tree_bin)
sbm.tree_bin
sbm.tree_bin$estimate()
sbm.tree_bin
## ----select SBM, echo=TRUE, eval = TRUE--------------------------------------------------------------------------------------------------------
Q = which.max(sbm.tree_bin$ICL)
Q
Q
## ----extract param SBM, echo=TRUE, eval = TRUE-------------------------------------------------------------------------------------------------
paramEstimSBM <- extractParamBM(sbm.tree_bin,Q)
paramEstimSBM$pi
paramEstimSBM$alpha
paramEstimSBM$Z
## ----plot org treeb bin,  echo=TRUE, eval = TRUE-----------------------------------------------------------------------------------------------
plotMatrix(tree_bin,'tree','tree', fileNameSave = NULL, clustering = list(row=paramEstimSBM$Z))
## ----tree tree bin plot, echo=TRUE-------------------------------------------------------------------------------------------------------------
plotMatrix(Mat = tree_bin,rowFG = 'tree', colFG  = 'tree')
## ----list names blocks,  echo=TRUE, eval = TRUE------------------------------------------------------------------------------------------------
lapply(1:Q,function(q){tree_list[paramEstimSBM$Z == q]})
################################################## SBM poisson
## ----tree tree plot, echo=TRUE-----------------------------------------------------------------------------------------------------------------
plotMatrix(Mat = tree,rowFG = 'tree', colFG  = 'tree')
## ----SBM Poisson, echo=TRUE, eval = TRUE-------------------------------------------------------------------------------------------------------
sbm.tree <- BM_poisson("SBM_sym",tree)
## ----estimate SBM poisson, echo=TRUE, eval = FALSE---------------------------------------------------------------------------------------------
sbm.tree$estimate()
## ----select SBM poisson, echo=TRUE, eval = TRUE------------------------------------------------------------------------------------------------
Q = which.max(sbm.tree$ICL)
Q
## ----extract param SBM poisson, echo=TRUE, eval = TRUE-----------------------------------------------------------------------------------------
paramEstimSBMPoisson <- extractParamBM(sbm.tree,Q)
paramEstimSBMPoisson$pi
paramEstimSBMPoisson$lambda
plotMatrix(tree,'tree','tree', fileNameSave = NULL, clustering = list(row=paramEstimSBMPoisson$Z))
par(mfrow = c(1,1))
G <- graph_from_adjacency_matrix(paramEstimSBMPoisson$lambda, mode = c("undirected"), weighted = TRUE, diag = TRUE)
plot.igraph(G,vertex.size=paramEstimSBMPoisson$pi*100,edge.width=abs(E(G)$weight)*3,vertex.color=1:Q, layout=layout_nicely)
## ----list names blocks Poisson,  echo=TRUE, eval = TRUE----------------------------------------------------------------------------------------
lapply(1:Q,function(q){tree_list[paramEstimSBMPoisson$Z == q]})
## ----alluvial, echo=TRUE,eval=TRUE-------------------------------------------------------------------------------------------------------------
A <- as.data.frame(table(paramEstimSBM$Z,paramEstimSBMPoisson$Z))
colnames(A)=c('SBM Bern',"SBM Poisson","Freq")
w   <- which(A$Freq != 0)
A <- A[w,]
alluvial(A[,c(1,2)],freq=A$Freq)
sbm.tree$memberships[[6]]
sbm.tree$memberships[[6]]$Z
sbm.cov <- BM_poisson_covariates("SBM_sym",adj = tree, covariates = ListVar)
sbm.cov$estimate()
## ----select SBM covar, echo=TRUE, eval = TRUE--------------------------------------------------------------------------------------------------
Q = which.max(sbm.cov$ICL)
Q
## ----extract param SBM poisson covar, echo=TRUE, eval = TRUE-----------------------------------------------------------------------------------
paramEstimSBMPoissonCov <- extractParamBM(sbm.cov,Q)
paramEstimSBMPoissonCov$pi
paramEstimSBMPoissonCov$lambda
paramEstimSBMPoissonCov$Z
paramEstimSBMPoissonCov$theta
## ----plot org tree  cov,  echo=TRUE, eval = TRUE-----------------------------------------------------------------------------------------------
plotMatrix(tree,'tree','tree', fileNameSave = NULL, clustering = list(row=paramEstimSBMPoissonCov$Z))
## ----list names blocks poisson cov,  echo=TRUE, eval = TRUE------------------------------------------------------------------------------------
lapply(1:Q,function(q){tree_list[paramEstimSBMPoissonCov$Z == q]})
## ----alluvial cov, echo=TRUE,eval=TRUE---------------------------------------------------------------------------------------------------------
B <- as.data.frame(table(paramEstimSBM$Z,paramEstimSBMPoisson$Z,paramEstimSBMPoissonCov$Z))
colnames(B) = c('SBM Bern',"SBM Poisson","SBM Poisson + Cov","Freq")
w   <- which(B$Freq!=0)
B <- B[w,]
alluvial(B[,c(1,2,3)],freq=B$Freq)
paramEstimSBMPoissonCov$theta
## ----tree fungis plot, echo=TRUE---------------------------------------------------------------------------------------------------------------
plotMatrix(Mat = fungi_tree,rowFG = 'fungi', colFG  = 'tree')
## ----LBM,echo=TRUE,eval=FALSE------------------------------------------------------------------------------------------------------------------
lbm <- BM_bernoulli("LBM",as.matrix(fungi_tree))
lbm$estimate()
## ----select lBM covar, echo=TRUE, eval = TRUE--------------------------------------------------------------------------------------------------
Q = which.max(lbm$ICL)
Q
paramEstimLBM <- extractParamBM(lbm,Q)
paramEstimLBM$Q
## ----extract param LBM, echo=TRUE, eval = TRUE-------------------------------------------------------------------------------------------------
paramEstimLBM$piRow
paramEstimLBM$piCol
paramEstimLBM$alpha
paramEstimLBM$ZRow
paramEstimLBM$ZRow
## ----plot org  tree fungis,  echo=TRUE, eval = TRUE--------------------------------------------------------------------------------------------
plotMatrix(fungi_tree,'fungi','tree', fileNameSave = NULL, clustering = list(row = paramEstimLBM$ZRow,col = paramEstimLBM$ZCol))
